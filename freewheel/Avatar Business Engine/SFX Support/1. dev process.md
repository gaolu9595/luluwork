# Dev Process

[TOC]

## Kafka异步生产者

### Kafka生产者机制详解

https://segmentfault.com/a/1190000037689992

### sarama生产者机制

https://juejin.cn/post/6866316565348876296#heading-3

### sarama生产者example

https://blog.csdn.net/chinawangfei/article/details/93097203





## 使用Channel来做handler线程和producer线程之间的通信

### channel基础使用

https://juejin.cn/post/6844903940438360072

https://colobu.com/2016/04/14/Golang-Channels/#Channel%E7%B1%BB%E5%9E%8B

### select语句

https://www.kancloud.cn/digest/batu-go/153533

### channel关闭后，producer需要receive所有残留消息后再关闭

 https://blog.csdn.net/yzf279533105/article/details/98643301



## 问题：当channel sender write过快时，channel receiver和sender都会阻塞

压力测试：sender write的时间间隔为microseconed级别或者nanosecond级别时，channel读写会因Kafka Producer的阻塞而阻塞（去掉Kafka produce的操作，将不会阻塞，所以是Kafka Producer造成的阻塞）

进而：

1. 导致channel无法继续read
2. 导致channel无法继续write（channel中的遗留msg超过缓冲区大小），handler线程会阻塞，以至于主Consumer线程无法consume

排查：

1. 去掉Producer.Return.Success=true和Producer.Return.Error=true的配置以后（不再需要从Errors和Succeses这两个channel中读），发现Kafka Producer不再阻塞：说明阻塞的点不在于Kafka Producer的生产性能下降，而在于Kafka Producer生产消息成功或失败时的回调读被阻塞了

2. 开启Producer.Return.Success=true，并使用select监听Successes channel：当ExternalAdChan sender速度过快时，如果successes channel中的消息来不及被消费，则会死锁阻塞（极难排查，不一定能复现）

3. 开启Producer.Return.Success=false，并使用select监听Successes channel：当ExternalAdChan sender速度过快时，如果errors channel中并没有消息，也不会死锁阻塞

   ```golang
   	for {
   		select {
   		case msg, ok := <-ExternalAdChan:
   			count ++
   			// when channel has no remaining data and has been closed, 'ok' is false
   			if !ok {
   				log.Infof("channel has no remaining data and has been closed %s", entity.KAFKA)
   				return
   			}
   			log.Infof("!!!!!read %d external-ad-id from channel, %s!!!!!", count, time.Now().Format("2006-01-02 15:04:05"))
   			// 阻塞在producer这里
   			producerRecord := &sarama.ProducerMessage{
   				Topic: KafkaProducerTopicList[0],
   				Key:   nil,
   				Value: sarama.ByteEncoder(msg),
   			}
   			producer.Input() <- producerRecord
   		case <-producer.Errors():
   			log.Errorf("!!!!!err to send message into Kafka %s!!!", entity.KAFKA)
   		case <-producer.Successes():
   			log.Infof("!!!!!succ to send message into Kafka %s!!!", entity.KAFKA)
   		}
   	}
   ```

解决：

**新开启一个goroutine**来单独监听Successes和Errors channel，就不会阻塞原来的producer线程了。

因为select是随机取channel来监听，当msg channel、Successes channel、Errors channel在同一个select的时候，可能会死锁

```golang
	go func() {
		for {
			select {
			case <-producer.Successes():
				log.Infof("!!!!!!!!!!!!!!!")
			case err := <-producer.Errors():
				log.Errorf("%s producer err: %s %s", KafkaProducerTopicList[0], err.Err, entity.KAFKA)
			}
		}
	}()

	for {
		select {
		case msg, ok := <-ExternalAdChan:
			count ++
			// when channel has no remaining data and has been closed, 'ok' is false
			if !ok {
				log.Infof("channel has no remaining data and has been closed %s", entity.KAFKA)
				return
			}
			log.Infof("!!!!!read %d external-ad-id from channel, %s!!!!!", count, time.Now().Format("2006-01-02 15:04:05"))
			// 阻塞在producer这里
			producerRecord := &sarama.ProducerMessage{
				Topic: KafkaProducerTopicList[0],
				Key:   nil,
				Value: sarama.ByteEncoder(msg),
			}
			producer.Input() <- producerRecord
		}
	}
```

结论：

https://www.douyacun.com/article/49dc293a800651763fd9da24842904de

如果kafkaConfig.Producer.Return.Errors和kafkaConfig.Producer.Return.Successes设置为true，则一定要有相应的消费channel的代码，且必须及时消费，否则可能发生消息阻塞。**如无必要，尽量避免使用kafkaConfig.Producer.Return.Successes=true**



## 当异步生产者遇到一些Error导致Produce失败时，应该怎么处理？

1. 不能中断不能阻塞：因为ExternalAdProducer的阻塞会进而使ExternalAdProcessor和ExternalAdConsumer阻塞，这样的话线上的external-ads会无法被及时处理
2. 不能保存发送失败的external-ad-id：因为没办法获取到发送失败的这个msg内容
3. 拟采用方案：只要ExternalAdProcessor和ExternalAdConsumer运行正常，正常将external-ad写入了MySQL，SFX可以在问题修复后再重刷那一段bug时间内的没有处理的external-ad来做attributes update。



**因Kafka本身而非APP Code造成的问题，无法完全避免**

